raw_pca <- prcomp(clean_data %>% select(-Y), scale. = TRUE)
# Rotated scores = raw PCA scores * rotation matrix
rotated_scores <- as.matrix(raw_pca$x[, 1:ncol(pca_result$loadings)]) %*% as.matrix(pca_result$loadings)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(rotated_scores)
lowerCor(pca_data) # We include this step to ensure component scores are correct
pca_data$Y <- clean_data$Y  # Add the outcome variable back
# Perform PCA with varimax rotation
pca_result <- principal(
r = cor(clean_data %>% select(-Y)),  # Correlation matrix of predictors
nfactors = ncol(clean_data) - 1,     # Number of components to retain
rotate = "varimax",                  # Perform varimax rotation
scores = FALSE                       # Do not compute scores here
)
# Extract the loadings matrix
loadings_matrix <- as.matrix(pca_result$loadings)
# Center the data (but do not scale it)
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute rotated component scores: centered data times the loadings matrix
rotated_scores <- as.matrix(data_centered) %*% loadings_matrix
# Verify orthogonality of the rotated components
lowerCor(rotated_scores)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(rotated_scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
# Perform PCA with varimax rotation
pca_result <- principal(
r = cor(clean_data %>% select(-Y)),  # Correlation matrix of predictors
nfactors = ncol(clean_data) - 1,     # Number of components to retain
rotate = "varimax",                  # Perform varimax rotation
scores = FALSE                       # Do not compute scores here
)
# Extract the loadings matrix
loadings_matrix <- as.matrix(pca_result$loadings)
# Center the data (but do not scale it)
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute rotated component scores: centered data times the loadings matrix
scores <- as.matrix(data_centered) %*% loadings_matrix
# Verify orthogonality of the rotated components
lowerCor(scores)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
loadings_matrix
# Verify orthogonality of the rotated components
lowerCor(scores)
lowerCor(loadings_matrix)
# Perform PCA with varimax rotation
pca_result <- principal(
r = cor(clean_data %>% select(-Y)),  # Correlation matrix of predictors
nfactors = 3,     # Number of components to retain
rotate = "varimax",                  # Perform varimax rotation
scores = FALSE                       # Do not compute scores here
)
# Extract the loadings matrix
loadings_matrix <- as.matrix(pca_result$loadings)
# Center the data (but do not scale it)
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute rotated component scores: centered data times the loadings matrix
scores <- as.matrix(data_centered) %*% loadings_matrix
# Verify orthogonality of the rotated components
lowerCor(scores)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
pca_result$weights
# Perform PCA with varimax rotation
pca_result <- principal(
r = cor(clean_data %>% select(-Y)),  # Correlation matrix of predictors
nfactors = ncol(clean_data) - 1,     # Number of components to retain
rotate = "varimax",                  # Perform varimax rotation
scores = TRUE                        # Ask for scores
)
# Center the data manually
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute PCA scores correctly: center the data and use the rotation loadings
rotated_scores <- as.matrix(data_centered) %*% as.matrix(pca_result$weights)
# Verify the orthogonality of rotated components
lowerCor(rotated_scores)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(rotated_scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
# Perform PCA with varimax rotation
pca_result <- principal(
r = cor(clean_data %>% select(-Y)),  # Correlation matrix of predictors
nfactors = 3,     # Number of components to retain
rotate = "varimax",                  # Perform varimax rotation
scores = TRUE                        # Ask for scores
)
# Center the data manually
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute PCA scores correctly: center the data and use the rotation loadings
rotated_scores <- as.matrix(data_centered) %*% as.matrix(pca_result$weights)
# Verify the orthogonality of rotated components
lowerCor(rotated_scores)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(rotated_scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
pca_result$loadings
# Perform PCA with varimax rotation
pca_result <- principal(
r = cor(clean_data %>% select(-Y)),  # Correlation matrix of predictors
nfactors = 3,     # Number of components to retain
rotate = "none",                  # Perform varimax rotation
scores = TRUE                        # Ask for scores
)
# Center the data manually
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute PCA scores correctly: center the data and use the rotation loadings
rotated_scores <- as.matrix(data_centered) %*% as.matrix(pca_result$weights)
# Verify the orthogonality of rotated components
lowerCor(rotated_scores)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(rotated_scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
# Perform PCA with varimax rotation
pca_result <- principal(
clean_data %>% select(-Y),
nfactors = 3,     # Number of components to retain
rotate = "none",                  # Perform varimax rotation
scores = TRUE                        # Ask for scores
)
# Center the data manually
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute PCA scores correctly: center the data and use the rotation loadings
rotated_scores <- as.matrix(data_centered) %*% as.matrix(pca_result$weights)
# Verify the orthogonality of rotated components
lowerCor(rotated_scores)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(rotated_scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
cor(clean_data %>% select(-Y))
# Perform PCA with prcomp (no rotation, inherently orthogonal)
raw_pca <- prcomp(clean_data %>% select(-Y), scale. = TRUE, center = TRUE)
# Extract scores (already orthogonal)
rotated_scores <- raw_pca$x
# Verify orthogonality of components
lowerCor(rotated_scores)
# Combine scores with outcome variable
pca_data <- as.data.frame(rotated_scores)
pca_data$Y <- clean_data$Y
# Perform PCA with varimax rotation
pca_result <- principal(
clean_data %>% select(-Y),
nfactors = 3,     # Number of components to retain
rotate = "varimax",                  # Perform varimax rotation
scores = F                        # Ask for scores
)
# Center the data manually
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute PCA scores correctly: center the data and use the rotation loadings
rotated_scores <- as.matrix(data_centered) %*% as.matrix(pca_result$weights)
# Verify the orthogonality of rotated components
lowerCor(rotated_scores)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(rotated_scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
View(rotated_scores)
lowerCor(pca_result$loadings)
# Perform PCA with varimax rotation
pca_result <- principal(
clean_data %>% select(-Y),
nfactors = 3,     # Number of components to retain
rotate = "varimax",                  # Perform varimax rotation
scores = F                        # Ask for scores
)
# Center the data manually
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute PCA scores correctly: center the data and use the rotation loadings
rotated_scores <- as.matrix(data_centered) %*% as.matrix(pca_result$weights)
# Verify the orthogonality of rotated components
lowerCor(rotated_scores)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(rotated_scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
lowerCor(pca_result$loadings)
# Perform PCA with varimax rotation
pca_result <- principal(
clean_data %>% select(-Y),
nfactors = 3,     # Number of components to retain
rotate = "varimax",                  # Perform varimax rotation
scores = F                        # Ask for scores
)
# Center the data manually
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute PCA scores correctly: center the data and use the rotation loadings
rotated_scores <- as.matrix(data_centered) %*% as.matrix(pca_result$loadings)
# Verify the orthogonality of rotated components
lowerCor(rotated_scores)
# Combine rotated scores into a data frame
pca_data <- as.data.frame(rotated_scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
# Perform PCA with varimax rotation
pca_result <- principal(
clean_data %>% select(-Y),
nfactors = 3,       # Number of components to retain
rotate = "varimax", # Varimax rotation
scores = FALSE      # Do not compute scores automatically
)
# Center the data manually (mean subtraction)
data_centered <- scale(clean_data %>% select(-Y), scale = FALSE)
# Compute rotated scores using weights
rotated_scores <- as.matrix(data_centered) %*% as.matrix(pca_result$weights)
# Verify orthogonality or correlations of components
lowerCor(rotated_scores)
# Combine rotated scores with the outcome variable
pca_data <- as.data.frame(rotated_scores)
pca_data$Y <- clean_data$Y  # Add the outcome variable back
library(terra)
# Define the required packages
required_packages <- c("leaflet", "dplyr", "rnaturalearth", "rnaturalearthdata", "stringr", "terra")
# Identify packages that are not yet installed
missing_packages <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
# Install missing packages
if(length(missing_packages)) {
install.packages(missing_packages, dependencies = F)
}
# Load all required packages
lapply(required_packages, library, character.only = TRUE)
# Define the languages and corresponding countries
languages <- list(
"English" = c(
"United States", "United Kingdom", "Canada", "Australia", "New Zealand",
"Ireland", "South Africa", "India", "Nigeria", "Philippines", "Singapore",
"Kenya", "Ghana", "Uganda", "Tanzania", "Zimbabwe", "Malta", "Barbados",
"Jamaica", "Trinidad and Tobago"
),
"French" = c(
"France", "Canada", "Belgium", "Switzerland", "Luxembourg", "Senegal",
"Ivory Coast", "Democratic Republic of the Congo", "Madagascar",
"Cameroon", "Burkina Faso", "Niger", "Mali", "Rwanda", "Burundi",
"Benin", "Togo", "Republic of the Congo", "Chad", "Guinea"
),
"German" = c(
"Germany", "Austria", "Switzerland", "Luxembourg",
"Liechtenstein", "Namibia"
),
"Dutch" = c(
"Netherlands", "Belgium", "Suriname", "Aruba", "Curacao",
"Sint Maarten", "Indonesia"
)
)
# Create a data frame with language and country
language_df <- data.frame(
Country = unlist(languages),
Language = rep(names(languages), times = sapply(languages, length)),
stringsAsFactors = FALSE
)
# Remove duplicates
language_df <- language_df %>%
distinct()
# Load country polygons
world <- ne_countries(scale = "medium", returnclass = "sf")
# Identify unmatched countries before correction
unmatched_countries <- setdiff(language_df$Country, world$name)
# Manually correct country names to match 'world$name'
country_corrections <- c(
"United States" = "United States of America",
"Ivory Coast" = "Côte d'Ivoire",
"Democratic Republic of the Congo" = "Dem. Rep. Congo",
"Republic of the Congo" = "Congo",
"Curacao" = "Curaçao"
)
# Apply corrections
language_df$Country <- recode(language_df$Country, !!!country_corrections)
# Identify any remaining unmatched countries
unmatched_countries_after <- setdiff(language_df$Country, world$name)
# Merge language data with world map
world_lang <- world %>%
left_join(language_df, by = c("name" = "Country"))
# Replace NA with "None"
world_lang$Language[is.na(world_lang$Language)] <- "None"
# Create color palette
pal <- colorFactor(
palette = c("antiquewhite", "bisque3", "cornsilk4", "orange", "lightgray"),
domain = c("English", "French", "German", "Dutch", "None")
)
# Initialize the leaflet map
leaflet(data = world_lang) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
# Add polygons
addPolygons(
fillColor = ~pal(Language),
weight = 1,
opacity = 1,
color = "white",
dashArray = "3",
fillOpacity = 0.7,
highlightOptions = highlightOptions(
weight = 3,
color = "#666",
dashArray = "",
fillOpacity = 0.7,
bringToFront = TRUE
),
label = ~paste0(name, ": ", ifelse(Language == "None", "No highlighted language", Language)),
labelOptions = labelOptions(
style = list("font-weight" = "normal", padding = "3px 8px"),
textsize = "15px",
direction = "auto"
)
) %>%
# Add legend
addLegend(
pal = pal,
values = ~Language,
opacity = 0.7,
title = "Languages",
position = "bottomright",
labFormat = labelFormat(
suffix = ""
)
)
# Define the required packages
required_packages <- c("leaflet", "dplyr", "rnaturalearth", "rnaturalearthdata", "stringr", "terra")
# Load all required packages
lapply(required_packages, library, character.only = T)
# Define the languages and corresponding countries
languages <- list(
"English" = c(
"United States", "United Kingdom", "Canada", "Australia", "New Zealand",
"Ireland", "South Africa", "India", "Nigeria", "Philippines", "Singapore",
"Kenya", "Ghana", "Uganda", "Tanzania", "Zimbabwe", "Malta", "Barbados",
"Jamaica", "Trinidad and Tobago"
),
"French" = c(
"France", "Canada", "Belgium", "Switzerland", "Luxembourg", "Senegal",
"Ivory Coast", "Democratic Republic of the Congo", "Madagascar",
"Cameroon", "Burkina Faso", "Niger", "Mali", "Rwanda", "Burundi",
"Benin", "Togo", "Republic of the Congo", "Chad", "Guinea"
),
"German" = c(
"Germany", "Austria", "Switzerland", "Luxembourg",
"Liechtenstein", "Namibia"
),
"Dutch" = c(
"Netherlands", "Belgium", "Suriname", "Aruba", "Curacao",
"Sint Maarten", "Indonesia"
)
)
# Create a data frame with language and country
language_df <- data.frame(
Country = unlist(languages),
Language = rep(names(languages), times = sapply(languages, length)),
stringsAsFactors = FALSE
)
# Remove duplicates
language_df <- language_df %>%
distinct()
# Load country polygons
world <- ne_countries(scale = "medium", returnclass = "sf")
# Identify unmatched countries before correction
unmatched_countries <- setdiff(language_df$Country, world$name)
# Manually correct country names to match 'world$name'
country_corrections <- c(
"United States" = "United States of America",
"Ivory Coast" = "Côte d'Ivoire",
"Democratic Republic of the Congo" = "Dem. Rep. Congo",
"Republic of the Congo" = "Congo",
"Curacao" = "Curaçao"
)
# Apply corrections
language_df$Country <- recode(language_df$Country, !!!country_corrections)
# Identify any remaining unmatched countries
unmatched_countries_after <- setdiff(language_df$Country, world$name)
# Merge language data with world map
world_lang <- world %>%
left_join(language_df, by = c("name" = "Country"))
# Replace NA with "None"
world_lang$Language[is.na(world_lang$Language)] <- "None"
# Create color palette
pal <- colorFactor(
palette = c("antiquewhite", "bisque3", "cornsilk4", "orange", "lightgray"),
domain = c("English", "French", "German", "Dutch", "None")
)
# Initialize the leaflet map
leaflet(data = world_lang) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
# Add polygons
addPolygons(
fillColor = ~pal(Language),
weight = 1,
opacity = 1,
color = "white",
dashArray = "3",
fillOpacity = 0.7,
highlightOptions = highlightOptions(
weight = 3,
color = "#666",
dashArray = "",
fillOpacity = 0.7,
bringToFront = TRUE
),
label = ~paste0(name, ": ", ifelse(Language == "None", "No highlighted language", Language)),
labelOptions = labelOptions(
style = list("font-weight" = "normal", padding = "3px 8px"),
textsize = "15px",
direction = "auto"
)
) %>%
# Add legend
addLegend(
pal = pal,
values = ~Language,
opacity = 0.7,
title = "Languages",
position = "bottomright",
labFormat = labelFormat(
suffix = ""
)
)
# Define the required packages
library("leaflet")
library("dplyr")
library("rnaturalearth")
library("rnaturalearthdata")
library("stringr")
library("terra")
# Define the languages and corresponding countries
languages <- list(
"English" = c(
"United States", "United Kingdom", "Canada", "Australia", "New Zealand",
"Ireland", "South Africa", "India", "Nigeria", "Philippines", "Singapore",
"Kenya", "Ghana", "Uganda", "Tanzania", "Zimbabwe", "Malta", "Barbados",
"Jamaica", "Trinidad and Tobago"
),
"French" = c(
"France", "Canada", "Belgium", "Switzerland", "Luxembourg", "Senegal",
"Ivory Coast", "Democratic Republic of the Congo", "Madagascar",
"Cameroon", "Burkina Faso", "Niger", "Mali", "Rwanda", "Burundi",
"Benin", "Togo", "Republic of the Congo", "Chad", "Guinea"
),
"German" = c(
"Germany", "Austria", "Switzerland", "Luxembourg",
"Liechtenstein", "Namibia"
),
"Dutch" = c(
"Netherlands", "Belgium", "Suriname", "Aruba", "Curacao",
"Sint Maarten", "Indonesia"
)
)
# Create a data frame with language and country
language_df <- data.frame(
Country = unlist(languages),
Language = rep(names(languages), times = sapply(languages, length)),
stringsAsFactors = FALSE
)
# Remove duplicates
language_df <- language_df %>%
distinct()
# Load country polygons
world <- ne_countries(scale = "medium", returnclass = "sf")
# Identify unmatched countries before correction
unmatched_countries <- setdiff(language_df$Country, world$name)
# Manually correct country names to match 'world$name'
country_corrections <- c(
"United States" = "United States of America",
"Ivory Coast" = "Côte d'Ivoire",
"Democratic Republic of the Congo" = "Dem. Rep. Congo",
"Republic of the Congo" = "Congo",
"Curacao" = "Curaçao"
)
# Apply corrections
language_df$Country <- recode(language_df$Country, !!!country_corrections)
# Identify any remaining unmatched countries
unmatched_countries_after <- setdiff(language_df$Country, world$name)
# Merge language data with world map
world_lang <- world %>%
left_join(language_df, by = c("name" = "Country"))
# Replace NA with "None"
world_lang$Language[is.na(world_lang$Language)] <- "None"
# Create color palette
pal <- colorFactor(
palette = c("antiquewhite", "bisque3", "cornsilk4", "orange", "lightgray"),
domain = c("English", "French", "German", "Dutch", "None")
)
# Initialize the leaflet map
leaflet(data = world_lang) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
# Add polygons
addPolygons(
fillColor = ~pal(Language),
weight = 1,
opacity = 1,
color = "white",
dashArray = "3",
fillOpacity = 0.7,
highlightOptions = highlightOptions(
weight = 3,
color = "#666",
dashArray = "",
fillOpacity = 0.7,
bringToFront = TRUE
),
label = ~paste0(name, ": ", ifelse(Language == "None", "No highlighted language", Language)),
labelOptions = labelOptions(
style = list("font-weight" = "normal", padding = "3px 8px"),
textsize = "15px",
direction = "auto"
)
) %>%
# Add legend
addLegend(
pal = pal,
values = ~Language,
opacity = 0.7,
title = "Languages",
position = "bottomright",
labFormat = labelFormat(
suffix = ""
)
)
